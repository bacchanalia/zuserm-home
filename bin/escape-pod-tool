#!/usr/bin/perl
use strict;
use warnings;
use utf8;
use List::Util qw(max);

my $baseUrl = "http://escapepod.org";
my $firstUrl = "$baseUrl/2005/05/12/ep001-imperial-by-jonathon-sullivan/";
my $wikiUrl = 'https://en.wikipedia.org/wiki/List_of_Escape_Pod_episodes';
my $forumUrl = "http://forum.escapeartists.net/index.php";

my $baseDir = "$ENV{HOME}/escapepod";
my $htmlCacheDir = "$baseDir/html-cache";
my $urlOverridesFile = "$baseDir/url-overrides";

my $mp3Dir = "$ENV{HOME}/Desktop/Music/books/Escape Pod";
my $csvDelim = ';';

sub parseHtml($$$);
sub htmlCacheFile($);
sub ensureHtmlCache($);
sub checkHtmlCache($);
sub attemptPutHtmlCache($);
sub refetchHtmlCache($);
sub attemptGunzip($);
sub readHtmlCache($);
sub getMP3Url($);
sub browserLoadArticle($);
sub ensureArticleUrlCache();
sub getMissingArticleUrls($);
sub getAllUrls();
sub getNextUrl($);
sub getArticleUrls();
sub getUrlOverrides($);
sub readAttCache($);
sub writeAttCache($$);
sub extraInfo();
sub personLinks();
sub fmtPersonLink($$);
sub crawlForum();
sub downloadMP3File($$$$);
sub tagMP3File($$);
sub newMP3FileName($);

sub csv($);
sub cell($);

sub arrEquals($$);
sub uniqArr(@);
sub run(@);
sub tryrun(@);

my @ratings = (
  "G"     => 'OK for Kids',
  "PG"    => '10 and Up',
  "PG-13" => '13 and Up',
  "R"     => '17 and Up',
  "X"     => 'Erotica - NOT FOR KIDS',
);
my %ratingCategories = @ratings;
my @ratingOrder = @ratings[grep { !($_ & 1) } 0 .. $#ratings];

my $cmds = join "|", qw(
  --csv --url --wiki --mp3url --mp3filename --download --download-only --tag
);
my $usage = "Usage:
  $0 [$cmds [EPNUM EPNUM ..]]
    Crawl $baseUrl and print information about each epiosde.
    HTML is fetched using curl/wget and cached in $baseDir/html-cache

    --csv {default if no args specified}
      Print episode info in CSV with delim=$csvDelim
    --url
      Print the article url for each EPNUM
    --wiki
      Use article HTML pages and forums to generate output in the same format as
        the wikipedia page at $wikiUrl.
      Prints to stdout and to the file $baseDir/wiki-out, and copies to the clipboard.
        {runs 'cat $baseDir/wiki-out | xsel -b'}
    --mp3url
      Print the download URL, parsed from the escape pod article HTML.
    --mp3filename
      Same as --mp3url, but only print the filename
    --download
      Get mp3url and download it with axel, then --tag it as below.
      Skip an EPNUM if the mp3filename
        or a file named \"###EPNUM TITLE.mp3\"
        is already present in the current dir.
    --download-only
      Same as --download, except do not --tag after downloading
    --tag
      Set tags and rename files.
      Gets mp3filename, assume it is in current dir.
      If mp3filename is not present, use the target name.

      Remove all id3 tags from the file, and set new tags.
      Uses 'id3v2' and 'mid3iconv'.
        title = \"TITLE\"
        author = \"AUTHOR [READER]\"
        tracknumber = \"EPNUM\"
        album = \"Escape Pod\"
      Rename the file from mp3filename => \"###EPNUM TITLE.mp3\"
      {with /s removed}

  EPNUM: escape pod episode number. if none are specified, all are used.
";

sub main(@){
  my $cmd = shift;
  $cmd = '--csv' if not defined $cmd;
  die $usage if $cmd !~ /^($cmds)$/;

  my @nums;
  for my $num(@_){
    die $usage if $num !~ /^(\d+)([ab]?)$/;
    my $epNum = sprintf "%03d%s", $1+0, $2;
    push @nums, $epNum;
  }
  die $usage if (grep {$_ !~ /^\d+[ab]?$/} @nums) > 0;

  my %okNums = map {$_ => 1} @nums;

  ensureArticleUrlCache();

  my $articleUrls = readAttCache 'article-url-cache';
  my @epNums = @nums;
  @epNums = sort keys %$articleUrls if @nums == 0;

  my $epInfo = {};
  my $extraInfo = extraInfo();
  for my $epNum(@epNums){
    my $url = $$articleUrls{$epNum};
    die "Missing article url for $epNum\n" if not defined $url;
    my $ex = $$extraInfo{$epNum};
    $$epInfo{$epNum} = sub{parseHtml $epNum, $url, $ex};
  }

  my $exitCode = 0;

  if($cmd eq '--csv'){
    for my $epNum(sort keys %$epInfo){
      my $info = &{$$epInfo{$epNum}}();
      print csv($info) . "\n";
    }
  }elsif($cmd eq '--url'){
    for my $epNum(sort keys %$epInfo){
      print "$$articleUrls{$epNum}\n";
    }
  }elsif($cmd eq "--wiki"){
    if(@nums == 0){
      for my $num(keys %$articleUrls){
        die $usage if $num !~ /^(\d+)([ab]?)$/;
        push @nums, sprintf "%03d%s", $1+0, $2;
      }
      @nums = sort @nums;
    }

    my $isAllEps = @nums == keys %$articleUrls;
    my $prefixFile = "$baseDir/wiki-prefix";
    my $suffixFile = "$baseDir/wiki-suffix";

    open OUT, "> $baseDir/wiki-out";

    if($isAllEps and -e $prefixFile){
      my $prefix = `cat $prefixFile`;
      print $prefix;
      print OUT $prefix;
    }

    for my $epNum(reverse @nums){
      my $articleUrl = $$articleUrls{$epNum};
      my $personLinks = personLinks();
      if(ensureHtmlCache $articleUrl){
        my $info = &{$$epInfo{$epNum}}();

        my $rating = $$info{rating};
        $rating = "" if $rating eq "?";
        $rating =~ s/^\s*//;
        $rating =~ s/\s*$//;

        my $forum;
        if($$info{forum} ne "?"){
          $forum = "[$forumUrl?topic=$$info{forum} EPF]";
        }else{
          $forum = "none";
        }

        die "Malformed episode number: $epNum\n" if $epNum !~ /^(\d+)([ab]?)$/;
        my $num = sprintf "%d%s", $1+0, $2;

        my $note = '';
        $note = " $$info{note}" if defined $$info{note};
        my $fmt = ''
          . "|-\n| " . ($num)
          . " || [$articleUrl $$info{title}]$note"
          . " || " . fmtPersonLink($$info{author}, $personLinks)
          . " || " . fmtPersonLink($$info{reader}, $personLinks)
          . " || $rating"
          . " || $$info{duration}"
          . " || $forum"
          . "\n"
          ;
        $fmt =~ s/&#8217;/'/g;
        $fmt =~ s/&amp;/&/g;
        print $fmt;
        print OUT $fmt;
      }else{
        print STDERR "   ERROR: $epNum\n";
      }
    }
    if($isAllEps and -e $suffixFile){
      my $suffix = `cat $suffixFile`;
      print $suffix;
      print OUT $suffix;
    }
    close OUT;
    run "cat $baseDir/wiki-out | xsel -b";
  }elsif($cmd =~ /^(--mp3url|--mp3filename|--download|--download-only|--tag)/){
    chdir $mp3Dir;
    $ENV{PWD} = $mp3Dir;
    for my $epNum(sort keys %$epInfo){
      my $articleUrl = $$epInfo{$epNum}{articleUrl};
      if(ensureHtmlCache $articleUrl){
        my $mp3Url = getMP3Url $articleUrl;
        my $mp3FileName = $1 if $mp3Url =~ /([^\/]*\.mp3)$/;
        if($cmd eq '--mp3url'){
          print "$mp3Url\n";
        }elsif($cmd eq '--mp3filename'){
          print "$mp3FileName\n";
        }elsif($cmd eq '--download'){
          downloadMP3File($mp3Url, $mp3FileName, $$epInfo{$epNum}, 1);
        }elsif($cmd eq '--download-only'){
          downloadMP3File($mp3Url, $mp3FileName, $$epInfo{$epNum}, 0);
        }elsif($cmd eq '--tag'){
          tagMP3File($mp3FileName, $$epInfo{$epNum});
        }
      }else{
        print "   ERROR: $epNum\n";
        $exitCode = 1;
      }
    }
  }
  exit $exitCode;
}

sub parseHtml($$$){
  my ($epNum, $url, $ex) = @_;
  my $html = readHtmlCache $url;

  my $info = {};
  $$info{number} = $epNum;
  $$info{articleUrl} = $url;
  $$info{date} = $1 if $url =~ /^$baseUrl\/(\d+\/\d+\/\d+)/;

  my @atts = qw(title author reader rating duration forum);

  my $mp3 = `ls "$mp3Dir"/$epNum*.mp3 2>/dev/null`;
  chomp $mp3;

  my $b = "(?:<\\/?\\s*(?:b|strong)[^>]*>)";
  my $span = "(?:<\\/?\\s*span[^>]*>)";
  my $ws = "[ \\t\\n]";
  my $readBy = "(?:narrated|read)$ws+(?:and produced )?by:?$ws+";

  my $isFlash = $html =~ /<title>.*Flash.*<\/title>/i;

  $html =~ s/\xa0/ /g;
  $html =~ s/\xc2/ /g;
  $html =~ s/&nbsp;/ /g;
  $html =~ s/&#160;/ /g;
  $html =~ s/Â/ /g;

  my $htmlNoMeta = $html;
  $htmlNoMeta =~ s/<meta$ws*[^>]*>//g;

  if(defined $$ex{note}){
    $$info{note} = $$ex{note};
  }

  if(defined $$ex{title}){
    $$info{title} = $$ex{title};
  }elsif($html =~ /<title>(.*)<\/title>/i){
    my $title = $1;
    utf8::decode $title;
    $title =~ s/[:\.\-]\s*Escape\s*Pod\s*//g;
    $title =~ s/^\s*(EP|Escape\s*Pod|Episode)\s*\d+[ab]?[:\-]?//i;
    $title =~ s/^\s*//;
    $title =~ s/\s*$//;
    $title =~ s/&#039;/'/g;
    $title =~ s/&amp;/&/g;
    $title =~ s/’/'/g;
    $title =~ s/“/"/g;
    $title =~ s/”/"/g;
    $title =~ s/&quot;/"/g;
    $title =~ s/^\s*-\s*//g;
    utf8::encode $title;
    $$info{title} = $title;
  }

  if(not $isFlash){
    if(defined $$ex{author}){
      $$info{author} = $$ex{author};
    }elsif($htmlNoMeta =~ />by$ws*:?\s*$span?$b?(?:<a [^>]*>)?$b?([^<(]+)/i){
      $$info{author} = $1;
    }
  }else{
    my @authors;
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /$b$ws*by$ws+([^<(]*)$b/gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /$b$ws*by$ws+([^<(]*)</gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /<\/a>,$ws*by$ws+([^<(]*)</gi;
    }
    if(@authors == 0){
      @authors = $htmlNoMeta =~ /(?<!narrated)$ws+by$ws+([^<(]*)</gi;
    }
    if(defined $$ex{author}){
      $$info{author} = $$ex{author};
    }else{
      $$info{author} = join ', ', @authors if @authors > 0;
    }
  }

  if(not $isFlash){
    if(defined $$ex{reader}){
      $$info{reader} = $$ex{reader};
    }elsif($htmlNoMeta =~ /$readBy$span?$b?(?:<a [^>]*>)?$b?([^<(]+)/i){
      $$info{reader} = $1;
    }
  }else{
    my @readers;
    if(@readers == 0){
      @readers = $htmlNoMeta =~ /$readBy(?:<a$ws+[^>]*>)?([^<]+)/gi;
    }
    if(@readers == 0){
      @readers = $htmlNoMeta =~ /\(narrator-? ([^)]+)\)/gi;
    }
    if(defined $$ex{reader}){
      $$info{reader} = $$ex{reader};
    }else{
      $$info{reader} = join ', ', @readers if @readers > 0;
    }
  }

  if(defined $$info{reader} and $$info{reader} =~ /^\s*the\s*author\s*$/i){
    $$info{reader} = $$info{author};
  }

  if(defined $$ex{rating}){
    $$info{rating} = $$ex{rating};
  }else{
    my $okRatings = join "|", reverse sort @ratingOrder;

    if(not defined $$info{rating}){
      if($html =~ /Rated\s*$b?\s*($okRatings)[ \t\n\.:\-<&]/){
        $$info{rating} = $1;
      }
    }

    if(not defined $$info{rating}){
      for my $rating(@ratingOrder){
        my $cat = $ratingCategories{$rating};
        if($html =~ /Rated\s*$b?\s*$cat[ \t\n\.:\-<&]/i){
          $$info{rating} = $rating;
        }
      }
    }

    if(not defined $$info{rating}){
      for my $rating(@ratingOrder){
        my $cat = $ratingCategories{$rating};
        die "$rating" if not defined $cat;
        if($html =~ /<meta property="article:section" content="$cat"\s*\/>/){
          $$info{rating} = $rating;
          last;
        }
      }
    }
  }

  my $durCache = readAttCache 'duration-cache';
  if(defined $$durCache{$epNum}){
    $$info{duration} = $$durCache{$epNum};
  }elsif($mp3 =~ /^(.*\/)?$epNum[^\/]*\.mp3$/ and -e $mp3){
    $mp3 =~ s/"/\\"/g;
    my $dur = `duration -n "$mp3"`;
    chomp $dur;
    if($dur =~ /^(\d+:)+\d+$/){
      $$info{duration} = $dur;
      $$durCache{$epNum} = $dur;
      writeAttCache 'duration-cache', $durCache;
    }
  }elsif($html =~ /<span[^>]*>\s*\[\s*((?:\d+:)+\d+)\s*\]\s*<\/span>/){
    $$info{duration} = $1;
  }

  my $forumCache = readAttCache 'forum-cache';
  if(defined $$ex{forum}){
    my $prev = $$forumCache{$epNum};
    if(not defined $prev or $prev ne $$ex{forum}){
      $$forumCache{$epNum} = $$ex{forum};
      writeAttCache 'forum-cache', $forumCache;
    }
  }
  if(not defined $$forumCache{$epNum}){
    print STDERR "updating forum cache\n";
    my $epArticles = crawlForum;
    for my $ep(sort keys %$epArticles){
      $$forumCache{$ep} = $$epArticles{$ep};
    }
    writeAttCache 'forum-cache', $forumCache;
  }
  if(defined $$forumCache{$epNum}){
    $$info{forum} = $$forumCache{$epNum};
  }

  for my $att(@atts){
    die "Missing '$att' for $epNum\n" if not defined $$info{$att};
  }

  return $info;
}

sub htmlCacheFile($){
  my $url = shift;
  my $name = $url;
  $name =~ s/^\Q$baseUrl\E//;
  $name =~ s/^\///;
  $name =~ s/\/$//;
  $name =~ s/\//%%/g;
  return "$htmlCacheDir/$name";
}

sub ensureHtmlCache($){
  my $url = shift;
  my $attempts;
  return 1 if checkHtmlCache $url;

  $attempts = 10;
  while($attempts > 0 and not checkHtmlCache $url){
    $attempts--;
    attemptPutHtmlCache $url;
  }
  if(not checkHtmlCache $url){
    browserLoadArticle $url;
  }
  $attempts = 10;
  while($attempts > 0 and not checkHtmlCache $url){
    $attempts--;
    attemptPutHtmlCache $url;
  }

  return checkHtmlCache $url;
}

sub checkHtmlCache($){
  my $url = shift;
  my $cacheFile = htmlCacheFile $url;
  return 0 if not -e $cacheFile;
  my $html = `cat $cacheFile`;
  if($html =~ /<html/){
    return 1;
  }else{
    return 0;
  }
}

sub attemptPutHtmlCache($){
  my $url = shift;

  my $cacheFile = htmlCacheFile $url;
  return 1 if checkHtmlCache $url;

  run "curl -L \"$url\" -o \"$cacheFile\" 2>/dev/null";
  return 1 if checkHtmlCache $url;
  attemptGunzip($cacheFile);
  return 1 if checkHtmlCache $url;

  run "wget \"$url\" -O \"$cacheFile\" 2>/dev/null";
  return 1 if checkHtmlCache $url;
  attemptGunzip($cacheFile);
  return 1 if checkHtmlCache $url;

  run "rm", "-f", $cacheFile;
  return 0;
}

sub refetchHtmlCache($){
  my $url = shift;

  my $file = htmlCacheFile $url;
  my $tmp = "$file.tmp";

  run "mv", $file, $tmp;
  if(ensureHtmlCache $url){
    system "rm", $tmp;
  }else{
    system "mv", $tmp, $file;
  }
}

sub attemptGunzip($){
  my $file = shift;
  if(-e $file){
    run "mv", $file, "$file.gz";
    tryrun "gunzip", "$file.gz";
  }
  if(-e "$file.gz"){
    run "rm", "-f", "$file.gz";
  }
}

sub readHtmlCache($){
  my $url = shift;
  my $cacheFile = htmlCacheFile $url;
  die "Missing html file for $url\n" if not checkHtmlCache $url;
  open FH, "< $cacheFile" or die "failed to read $cacheFile\n";
  my @lines = <FH>;
  close FH;
  return join '', @lines;
}

sub browserLoadArticle($){
  my $articleUrl = shift;
  system "uzbl $articleUrl >/dev/null 2>/dev/null &";
  sleep 5;
  system "pkill", "-f", "uzbl.*$articleUrl";
}

sub ensureArticleUrlCache(){
  my $cache = readAttCache 'article-url-cache';
  my @missing = getMissingArticleUrls $cache;
  if(keys %$cache == 0 or @missing > 0){
    $cache = getArticleUrls();
    @missing = getMissingArticleUrls $cache;
  }
  if(@missing > 0){
    print STDERR "Missing articles:\n" . join("\n", @missing) . "\n";
  }
  if(keys %$cache == 0 or @missing > 0){
    die "article url cache is incomplete\n";
  }
  writeAttCache 'article-url-cache', $cache;
  return $cache;
}

sub getMissingArticleUrls($){
  my $cache = shift;
  my @epNums = keys %$cache;
  return () if @epNums == 0;
  my @numericEpNums = map {s/[^0-9]+//g; 0+$_} @epNums;
  my %okNumericEpNums = map {$_ => 1} @numericEpNums;
  my $max = max @numericEpNums;
  my @expectedNums = (1..$max);
  my @missing = grep {not defined $okNumericEpNums{$_}} @expectedNums;
  return @missing;
}

sub getAllUrls(){
  my $url = $firstUrl;
  my @urls;
  while(defined $url){
    attemptPutHtmlCache $url;
    push @urls, $url;
    my $nextUrl = getNextUrl($url);
    if(not defined $nextUrl){
      print "\nrefetching $url\n";
      refetchHtmlCache $url;
      $nextUrl = getNextUrl($url);
    }
    $url = $nextUrl;
  }
  return @urls;
}

sub getNextUrl($){
  my $url = shift;
  my $html = readHtmlCache $url;
  my @anchors = $html =~ /<a [^>]+>/g;
  for my $anchor(@anchors){
    if($anchor =~ /rel="next"/ and $anchor =~ /href="([^"]+)"/){
      return $1;
    }
  }
  return undef;
}

sub getArticleUrls(){
  my $urlOverrides = getUrlOverrides($urlOverridesFile);
  my $articleUrls = {};
  my @errors;
  for my $url(getAllUrls()){
    my $num = undef;
    my $skip = 0;
    my $html = readHtmlCache $url;

    my $title = $1 if $html =~ /<title[^>]*>([^<]*)<\/title>/;

    my $regex = "(?:ep|escape ?pod|episode-?)-?(\\d+[a-b]?)";
    if(defined $title and $title =~ /$regex/i){
      $num = $1;
    }elsif($url =~ /$regex/){
      $num = $1;
    }

    if(defined $$urlOverrides{$url}){
      if($$urlOverrides{$url} =~ /^(skip)$/i){
        $skip = 1;
      }elsif($$urlOverrides{$url} =~ /^(\d+[ab]?)$/){
        $num = $1;
      }
    }

    if(defined $num and not $skip){
      die $usage if $num !~ /^(\d+)([ab]?)$/;
      my $epNum = sprintf "%03d%s", $1+0, $2;
      if(defined $$articleUrls{$epNum}){
        push @errors, "duplicate url: \n$$articleUrls{$epNum}\n$url\n";
      }
      $$articleUrls{$epNum} = $url;
    }
  }
  die @errors if @errors > 0;
  return $articleUrls;
}

sub getUrlOverrides($){
  my $file = shift;
  my @lines = `cat $file 2>/dev/null`;
  my $overrides = {};
  for my $line(@lines){
    if($line =~ /^(\d+[ab]?|skip) (.*)$/){
      $$overrides{$2} = $1;
    }
  }
  return $overrides;
}

sub readAttCache($){
  my $name = shift;
  my $cacheFile = "$baseDir/$name";
  my @lines = `cat $cacheFile 2>/dev/null`;
  my $cache = {};
  for my $line(@lines){
    if($line =~ /^(\d+[ab]?)\s+(.*)$/){
      $$cache{$1} = $2;
    }
  }
  return $cache;
}

sub writeAttCache($$){
  my ($name, $cache) = @_;
  my $cacheFile = "$baseDir/$name";
  my $s = '';
  open FH, "> $cacheFile" or die "Couldnt write to $cacheFile\n";
  for my $epNum(sort {$a cmp $b} keys %$cache){
    die "Malformed episode number: $epNum\n" if $epNum !~ /^(\d+)([ab]?)$/;
    print FH sprintf "%03d%s %s\n", $1, $2, $$cache{$epNum};
  }
  close FH;
}

sub extraInfo(){
  my $file = "$baseDir/extra-info";
  my $extraInfo = {};
  for my $line(`cat $file`){
    $line =~ s/#.*//;
    next if $line =~ /^\s*$/;
    if($line =~ /^(\d\d\d[ab]?) (title|author|reader|rating|forum|note) (.*)$/){
      $$extraInfo{$1}{$2} = $3;
    }else{
      die "malformed extra-info line: $line";
    }
  }
  return $extraInfo;
}

sub personLinks(){
  my $file = "$baseDir/person-links";
  my $personLinks = {};
  for my $line(`cat $file`){
    $line =~ s/#.*//;
    next if $line =~ /^\s*$/;
    if($line =~ /^(.*)=>(.*)$/){
      my ($person, $link) = ($1, $2);
      chomp $link;
      $person =~ s/^\s*//;
      $person =~ s/\s*$//;
      $link =~ s/^\s*//;
      $link =~ s/\s*$//;
      $$personLinks{$person} = $link;
    }else{
      die "malformed person-links line: $line";
    }
  }
  return $personLinks;
}

sub fmtPersonLink($$){
  my ($person, $links) = @_;
  return $person if $person =~ /[\[\]]/;

  $person =~ s/\s*\.\s*Music\s*by\s*$//i;
  $person =~ s/&#8217;/'/g;
  $person =~ s/&amp;/&/g;
  my @people = split /(?:(?:\s+|,)and\s+)|(?:\s*[&,]\s*)/, $person;
  @people = grep {defined $_ and $_ =~ /\w/} @people;
  @people = grep {not $_ =~ /^\s*e-?mail[ :-]*$/} @people;
  @people = uniqArr @people;

  my @fmtPeople;
  for my $p(@people){
    $p =~ s/\s*\.?\s*$//;
    $p =~ s/ of the$//;
    $p =~ s/ of$//;
    $p =~ s/^author //;
    $p =~ s/^and //;
    $p =~ s/^\s*:\s*//;
    $p =~ s/;\s*courtesy\s*$//;
    $p =~ s/^\s*//;
    $p =~ s/\s*$//;
    my $link = $$links{$p};
    my $fmt;
    if(not defined $link){
      $fmt = "[[$p]]";
    }elsif($link =~ /^[ a-zA-Z0-9_\-]+$/){
      $fmt = "[[$p ($link)|$p]]";
    }else{
      $fmt = "[$link $p]";
    }
    push @fmtPeople, $fmt;
  }

  return join ", ", @fmtPeople;
}

sub crawlForum(){
  my $page = 0;
  my $epArticles = {};
  my $prevArticles = [];
  while($page < 50){
    my $board = "1." . (20*$page);
    my $url = "$forumUrl?board=$board";
    my $html = `curl -L \"$url\" 2>/dev/null`;
    my $pageArticles = [];
    while($html =~ /<a href="[^"]*topic=(\d+\.\d+)">EP(\d+[ab]?)[^*]/g){
      $$epArticles{$2} = $1;
      push @$pageArticles, $2;
    }
    print "@$pageArticles\n";
    if(arrEquals $pageArticles, $prevArticles){
      print "REPEAT AT PAGE $page\n";
      last;
    }
    $prevArticles = $pageArticles;
    $page++;
  }
  return $epArticles;
}

sub getMP3Url($){
  my $articleUrl = shift;
  my $html = readHtmlCache $articleUrl;
  if($html !~ /href="([^"]*\.mp3)"/){
    die "Could not read mp3 url for $articleUrl\n";
  }
  return $1;
}

sub downloadMP3File($$$$){
  my ($mp3Url, $mp3FileName, $ep, $tagFile) = @_;
  my $newFileName = newMP3FileName $ep;
  if(-e $mp3FileName or -e $newFileName){
    print "  skipping $$ep{number}..\n";
  }else{
    run "axel", $mp3Url;
    tagMP3File $mp3FileName, $ep if $tagFile;
  }
}
sub tagMP3File($$){
  my ($mp3FileName, $ep) = @_;
  my $oldFileName = $mp3FileName;
  my $newFileName = newMP3FileName $ep;

  if(not -e $oldFileName and not -e $newFileName){
    print "   ERROR: missing $oldFileName or $newFileName\n";
    return;
  }
  $oldFileName = $newFileName if not -e $oldFileName;

  run "id3v2", "--delete-all", $oldFileName;

  my $num = 0;
  $num += $1 if $$ep{number} =~ /(\d+)/;

  run "id3v2",
    "--TIT2", $$ep{title},
    "--TPE1", $$ep{author},
    "--TPE2", $$ep{reader},
    "--TRCK", $num,
    "--TALB", "Escape Pod",
    "--TYER", $$ep{date},
    $oldFileName;

  run "mid3iconv", $oldFileName;

  if($oldFileName ne $newFileName){
    run "mv", "--no-clobber", $oldFileName, $newFileName;
  }
}
sub newMP3FileName($){
  my $ep = shift;
  my $newFileName = "$$ep{number} $$ep{title}.mp3";
  $newFileName =~ s/\//_/g;
  return $newFileName;
}

sub csv($){
  my $ep = shift;
  my @cols = (
    cell $$ep{number},
    cell $$ep{title},
    cell $$ep{author},
    cell $$ep{reader},
    cell $$ep{date},
    cell $$ep{articleUrl},
  );
  return join ($csvDelim, @cols);
}
sub cell($){
  my $cell = shift;
  $cell =~ s/\&amp;/\&/g;
  if($cell =~ /\Q$csvDelim\E/ or $cell =~ /\Q"\E/){
    $cell =~ s/"/""/g;
    $cell = "\"$cell\"";
    return $cell;
  }else{
    return $cell;
  }
}

sub uniqArr(@){
  my %seen;
  my @newArr;
  for my $elem(@_){
    push @newArr, $elem if not defined $seen{$elem};
    $seen{$elem} = 1;
  }
  return @newArr;
}
sub arrEquals($$){
  my ($arr1, $arr2) = @_;
  return 0 if not defined $arr1 or not defined $arr2;
  return 0 if @$arr1 != @$arr2;
  for(my $i=0; $i<@$arr1; $i++){
    return 0 if $$arr1[$i] ne $$arr2[$i];
  }
  return 1;
}

sub run(@){
  tryrun @_;
  die "error running @_\n" if $? != 0;
}
sub tryrun(@){
  print "@_\n";
  system @_;
}

&main(@ARGV);
